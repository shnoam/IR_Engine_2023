{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Install required packages"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!gcloud dataproc clusters list --region us-central1\n",
    "!pip install -q google-cloud-storage==1.43.0\n",
    "!pip install -q graphframes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!ls -l /usr/lib/spark/jars/graph*"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "some imports"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "from google.cloud import storage\n",
    "import math\n",
    "import hashlib\n",
    "def _hash(s):\n",
    "    return hashlib.blake2b(bytes(s, encoding='utf8'), digest_size=5).hexdigest()\n",
    "\n",
    "nltk.download('stopwords')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark import SparkContext, SparkConf,SparkFiles\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pyspark.shell import spark\n",
    "\n",
    "spark"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "create the connection to our bucket in GCP"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bucket_name = '316139070_206204588'\n",
    "full_path = f\"gs://{bucket_name}/\"\n",
    "paths=[]\n",
    "\n",
    "client = storage.Client()\n",
    "blobs = client.list_blobs(bucket_name)\n",
    "for b in blobs:\n",
    "    if b.name != 'graphframes.sh':\n",
    "        paths.append(full_path+b.name)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "parquetFile = spark.read.parquet(*paths)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%cd -q /home/dataproc\n",
    "!ls inverted_index_gcp.py"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pyspark.shell import sc\n",
    "\n",
    "sc.addFile(\"/home/dataproc/inverted_index_gcp.py\")\n",
    "sys.path.insert(0,SparkFiles.getRootDirectory())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Import MultiFile and InvertedIndex classes\n",
    "from inverted_index_gcp import *"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Extract data from wikipedia"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "parquetFile = spark.read.parquet(*paths)\n",
    "\n",
    "doc_text_pairs = parquetFile.select(\"id\", \"text\").rdd\n",
    "doc_title_pairs = parquetFile.select(\"id\",\"title\").rdd\n",
    "anchor_text_as_is= parquetFile.select(\"id\",\"anchor_text\").rdd"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# in order to rearrange and calculate the doc_anchor pairs\n",
    "doc_anchor_pairs=anchor_text_as_is.flatMap(lambda x:x[1]).groupByKey().mapValues(list).map(lambda x:(x[0],\" \".join([y for y in x[1]])))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Define stopwords by unite english and corpus stopwords"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "english_stopwords = frozenset(stopwords.words('english'))\n",
    "corpus_stopwords = [\"category\", \"references\", \"also\", \"external\", \"links\",\n",
    "                    \"may\", \"first\", \"see\", \"history\", \"people\", \"one\", \"two\",\n",
    "                    \"part\", \"thumb\", \"including\", \"second\", \"following\",\n",
    "                    \"many\", \"however\", \"would\", \"became\"]\n",
    "\n",
    "all_stopwords = english_stopwords.union(corpus_stopwords)\n",
    "\n",
    "RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){2,24}\"\"\", re.UNICODE) # regular expression\n",
    "\n",
    "def tokenize(text,gate):\n",
    "    list_of_tokens = [token.group() for token in RE_WORD.finditer(text.lower())]\n",
    "    if gate:        # WITHOUT stopwords\n",
    "            list_of_tokens = [token for token in list_of_tokens if token not in all_stopwords]\n",
    "    return list_of_tokens\n",
    "\n",
    "NUM_BUCKETS = 124\n",
    "def token2bucket_id(token):\n",
    "  return int(_hash(token),16) % NUM_BUCKETS\n",
    "def word_count(text, id, remove_stopword):\n",
    "    ''' Count the frequency of each word in `text` (tf) that is not included in\n",
    "    `all_stopwords` and return entries that will go into our posting lists.\n",
    "    Parameters:\n",
    "    -----------\n",
    "      text: str\n",
    "        Text of one document\n",
    "      id: int\n",
    "        Document id\n",
    "    Returns:\n",
    "    --------\n",
    "      List of tuples\n",
    "        A list of (token, (doc_id, tf)) pairs\n",
    "        for example: [(\"Anarchism\", (12, 5)), ...]\n",
    "    '''\n",
    "    tokens = tokenize(text,remove_stopword)\n",
    "    count_by_token = {}\n",
    "    for token in tokens:\n",
    "        if token not in all_stopwords:\n",
    "            if token not in count_by_token.keys():\n",
    "                count_by_token[token] = 1\n",
    "            else:\n",
    "                count_by_token[token] += 1\n",
    "    token_with_tuple = []\n",
    "    for token in count_by_token.keys():\n",
    "        token_with_tuple.append((token, (id, count_by_token[token])))\n",
    "    return token_with_tuple\n",
    "\n",
    "def doc_to_term_counter(text, exclude_stopword):\n",
    "  '''\n",
    "  Calculates word counter for a given document\n",
    "  '''\n",
    "  tokens = tokenize(text, exclude_stopword)\n",
    "  token_counter = Counter(tokens)\n",
    "  return token_counter\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Functions used during the building of the index"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def reduce_word_counts(unsorted_pl):\n",
    "  ''' Returns a sorted posting list by wiki_id.\n",
    "  Parameters:\n",
    "  -----------\n",
    "    unsorted_pl: list of tuples\n",
    "      A list of (wiki_id, tf) tuples\n",
    "  Returns:\n",
    "  --------\n",
    "    list of tuples\n",
    "      A sorted posting list.\n",
    "  '''\n",
    "  sorted_pl = sorted(unsorted_pl,key= lambda tup:tup[0])\n",
    "  return sorted_pl\n",
    "\n",
    "def calculate_df(postings):\n",
    "  ''' Takes a posting list RDD and calculate the df for each token.\n",
    "  Parameters:\n",
    "  -----------\n",
    "    postings: RDD\n",
    "      An RDD where each element is a (token, posting_list) pair.\n",
    "  Returns:\n",
    "  --------\n",
    "    RDD\n",
    "      An RDD where each element is a (token, df) pair.\n",
    "  '''\n",
    "  new_rdd = postings.map(lambda tok : (tok[0],len(tok[1]))) # posting[i]= (\"token\",[(doc_id1,tf1),(doc_id2,tf2)])\n",
    "  return new_rdd"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def calc_document_len(text, id,remove_stopwords):\n",
    "    tokens_lst = tokenize(text, remove_stopwords)\n",
    "\n",
    "    return (id, len(tokens_lst))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def partition_postings_and_write(postings,  storage_path):\n",
    "  ''' A function that partitions the posting lists into buckets, writes out \n",
    "  all posting lists in a bucket to disk, and returns the posting locations for \n",
    "  each bucket. Partitioning should be done through the use of `token2bucket` \n",
    "  above. Writing to disk should use the function  `write_a_posting_list`, a \n",
    "  static method implemented in inverted_index_colab.py under the InvertedIndex \n",
    "  class. \n",
    "  Parameters:\n",
    "  -----------\n",
    "    postings: RDD\n",
    "      An RDD where each item is a (w, posting_list) pair.\n",
    "    offset: int\n",
    "      The bucket number to start writing from.\n",
    "  Returns:\n",
    "  --------\n",
    "    RDD\n",
    "      An RDD where each item is a posting locations dictionary for a bucket. The\n",
    "      posting locations maintain a list for each word of file locations and \n",
    "      offsets its posting list was written to. See `write_a_posting_list` for \n",
    "      more details.\n",
    "  '''\n",
    "  # YOUR CODE HERE\n",
    "  return postings.map(lambda x: (token2bucket_id(x[0]),(x[0],x[1]))).groupByKey().map(lambda x: InvertedIndex.write_a_posting_list(x, bucket_name, storage_path))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# word counts on each one , for each term calc the tf\n",
    "word_counts_text = doc_text_pairs.flatMap(lambda x: word_count(x[1], x[0], True))\n",
    "word_counts_title = doc_title_pairs.flatMap(lambda x: word_count(x[1], x[0], False))\n",
    "word_counts_anchor = doc_anchor_pairs.flatMap(lambda x: word_count(x[1], x[0], False))\n",
    "\n",
    "# group by term and create a posting list (for each term) consists of ( doc_id ,tf ) pairs sorted by tf\n",
    "postings_text = word_counts_text.groupByKey().mapValues(reduce_word_counts)\n",
    "postings_title = word_counts_title.groupByKey().mapValues(reduce_word_counts)\n",
    "postings_anchor = word_counts_anchor.groupByKey().mapValues(reduce_word_counts)\n",
    "\n",
    "# filtering postings in text ( body)\n",
    "postings_filtered_text = postings_text.filter(lambda x: len(x[1])>50)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "calculate document frequency for text(body) , title , anchor"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "w2df_text = calculate_df(postings_filtered_text)\n",
    "w2df_title = calculate_df(postings_title)\n",
    "w2df_anchor = calculate_df(postings_anchor)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "calculate the following for the title in order to create title index"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "w2df_dict_title = w2df_title.collectAsMap()\n",
    "\n",
    "print('complete 1')\n",
    "document_len_title= doc_title_pairs.map(lambda tup: calc_document_len(tup[1],tup[0],False)).collectAsMap()\n",
    "print('complete 2')\n",
    "posting_locs_list_title = partition_postings_and_write(postings_title,\"title_index\").collect()\n",
    "print('complete 3')\n",
    "title_token_counter_by_doc_id = doc_title_pairs.map(lambda pair: (pair[0], doc_to_term_counter(pair[1],True)))\n",
    "print('complete 4')\n",
    "\n",
    "doc_id_to_norm_title = title_token_counter_by_doc_id.map(lambda docid_counter: (docid_counter[0], np.linalg.norm([(docid_counter[1][term] / document_len_title[docid_counter[0]]) * math.log(len(document_len_title) / w2df_dict_title.get(term, len(document_len_title)), 10) for term in docid_counter[1]]))).collectAsMap()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "doc_title_pairs_dict=doc_title_pairs.collectAsMap()     # define doc_title pairs , will be used in each index : (doc_id , title name), ..."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create the title index"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# collect all posting lists locations into one super-set\n",
    "super_posting_locs_title = defaultdict(list)\n",
    "for blob in client.list_blobs(bucket_name, prefix='postings_gcp/title_index'):\n",
    "  if not blob.name.endswith(\"pickle\"):\n",
    "    continue\n",
    "  with blob.open(\"rb\") as f:\n",
    "    posting_locs = pickle.load(f)\n",
    "    for k, v in posting_locs.items():\n",
    "      super_posting_locs_title[k].extend(v)\n",
    "# Create inverted index instance\n",
    "inverted_title = InvertedIndex()\n",
    "# Adding the posting locations dictionary to the inverted index\n",
    "inverted_title.posting_locs = super_posting_locs_title\n",
    "\n",
    "# Add the token - df dictionary to the inverted index\n",
    "inverted_title.df = w2df_dict_title\n",
    "\n",
    "#add document_len\n",
    "inverted_title.document_len = document_len_title\n",
    "\n",
    "# dictionary => {doc_id:title of the document}\n",
    "inverted_title.doc_id_title = doc_title_pairs_dict\n",
    "\n",
    "# for each document , calculate the norma of all tfidf values in this document\n",
    "# step 1 :  calculate tfidf for each term in first document:\n",
    "#            tf = frequency of term i in doc j divided by length of doc j\n",
    "#            idf = log base 10 of (number of docs in corpus divided by number of documents that term i appears in )\n",
    "# step 2 : for first document , compute norma using all tfidf value\n",
    "# step 3 : repeat for each docmunet\n",
    "#step 4 : create a dictionary with : key = doc_id and value = norma\n",
    "inverted_title.doc_id_to_norm = doc_id_to_norm_title\n",
    "\n",
    "inverted_title.write_index('.', 'title_index')\n",
    "# upload to gs\n",
    "index_src = \"title_index.pkl\"\n",
    "index_dst = f'gs://{bucket_name}/postings_gcp/{index_src}'\n",
    "!gsutil cp $index_src $index_dst"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "calculate the following for the anchor in order to create anchor index"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "w2df_dict_anchor = w2df_anchor.collectAsMap()\n",
    "\n",
    "print('complete 1')\n",
    "\n",
    "document_len_anchor= doc_anchor_pairs.map(lambda tup: calc_document_len(tup[1],tup[0],False)).collectAsMap()\n",
    "\n",
    "print('complete 2')\n",
    "\n",
    "posting_locs_list_anchor = partition_postings_and_write(postings_anchor,\"anchor_index\").collect()\n",
    "\n",
    "print('complete 3')\n",
    "\n",
    "anchor_token_counter_by_doc_id = doc_anchor_pairs.map(lambda pair: (pair[0], doc_to_term_counter(pair[1],True)))\n",
    "\n",
    "print('complete 4')\n",
    "anchor_token_counter_by_doc_id = anchor_token_counter_by_doc_id.filter(lambda x: document_len_anchor[x[0]] != 0) # keep only document in len > 0\n",
    "\n",
    "doc_id_to_norm_anchor = anchor_token_counter_by_doc_id.map(\n",
    "    lambda docid_counter: (docid_counter[0], np.linalg.norm([(docid_counter[1][term] / document_len_anchor.get(docid_counter[0],1)) * math.log(len(document_len_anchor) / w2df_dict_anchor.get(term, len(document_len_anchor)), 10) for term in docid_counter[1]]))).collectAsMap()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create the anchor index and write it to our bucket in GCP"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# collect all posting lists locations into one super-set\n",
    "super_posting_locs_anchor = defaultdict(list)\n",
    "for blob in client.list_blobs(bucket_name, prefix='postings_gcp/anchor_index'):\n",
    "  if not blob.name.endswith(\"pickle\"):\n",
    "    continue\n",
    "  with blob.open(\"rb\") as f:\n",
    "    posting_locs = pickle.load(f)\n",
    "    for k, v in posting_locs.items():\n",
    "      super_posting_locs_anchor[k].extend(v)\n",
    "# Create inverted index instance\n",
    "inverted_anchor = InvertedIndex()\n",
    "# Adding the posting locations dictionary to the inverted index\n",
    "inverted_anchor.posting_locs = super_posting_locs_anchor\n",
    "\n",
    "# Add the token - df dictionary to the inverted index\n",
    "inverted_anchor.df = w2df_dict_anchor\n",
    "\n",
    "#add document_len\n",
    "inverted_anchor.document_len = document_len_anchor\n",
    "\n",
    "# dictionary => {doc_id:title of the document}\n",
    "inverted_anchor.doc_id_title = doc_title_pairs_dict\n",
    "\n",
    "# for each document , calculate the norma of all tfidf values in this document\n",
    "# step 1 :  calculate tfidf for each term in first document:\n",
    "#            tf = frequency of term i in doc j divided by length of doc j\n",
    "#            idf = log base 10 of (number of docs in corpus divided by number of documents that term i appears in )\n",
    "# step 2 : for first document , compute norma using all tfidf value\n",
    "# step 3 : repeat for each docmunet\n",
    "#step 4 : create a dictionary with : key = doc_id and value = norma\n",
    "\n",
    "inverted_anchor.doc_id_to_norm = doc_id_to_norm_anchor\n",
    "\n",
    "inverted_anchor.write_index('.', 'anchor_index')\n",
    "# upload to gs\n",
    "index_src = \"anchor_index.pkl\"\n",
    "index_dst = f'gs://{bucket_name}/postings_gcp/{index_src}'\n",
    "!gsutil cp $index_src $index_dst"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "calculate the following for the text in order to create text index"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "w2df_dict_text = w2df_text.collectAsMap()\n",
    "\n",
    "print('complete 1')\n",
    "\n",
    "document_len_text= doc_text_pairs.map(lambda tup: calc_document_len(tup[1],tup[0],True)).collectAsMap()\n",
    "\n",
    "print('complete 2')\n",
    "\n",
    "# there are 124 buckets in each index\n",
    "posting_locs_list_text = partition_postings_and_write(postings_filtered_text,\"text_index\").collect()\n",
    "\n",
    "print('complete 3')\n",
    "\n",
    "text_token_counter_by_doc_id = doc_text_pairs.map(lambda pair: (pair[0], doc_to_term_counter(pair[1],True)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# for each document , calculate the norma of all tfidf values in this document\n",
    "# step 1 :  calculate tfidf for each term in first document:\n",
    "#            tf = frequency of term i in doc j divided by length of doc j\n",
    "#            idf = log base 10 of (number of docs in corpus divided by number of documents that term i appears in )\n",
    "# step 2 : for first document , compute norma using all tfidf value\n",
    "# step 3 : repeat for each docmunet\n",
    "#step 4 : create a dictionary with : key = doc_id and value = norma\n",
    "doc_id_to_norm_text = text_token_counter_by_doc_id.map(lambda docid_counter: (docid_counter[0], np.linalg.norm([(docid_counter[1][term] / document_len_text[docid_counter[0]]) * math.log(len(document_len_text) / w2df_dict_text.get(term, len(document_len_text)), 10) for term in docid_counter[1]]))).collectAsMap()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create the text index and write it to our bucket in GCP"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# collect all posting lists locations into one super-set\n",
    "super_posting_locs_text = defaultdict(list)\n",
    "for blob in client.list_blobs(bucket_name, prefix='postings_gcp/text_index'):\n",
    "  if not blob.name.endswith(\"pickle\"):\n",
    "    continue\n",
    "  with blob.open(\"rb\") as f:\n",
    "    posting_locs = pickle.load(f)\n",
    "    for k, v in posting_locs.items():\n",
    "      super_posting_locs_text[k].extend(v)\n",
    "\n",
    "# Create inverted index instance\n",
    "inverted_text = InvertedIndex()\n",
    "# Adding the posting locations dictionary to the inverted index\n",
    "inverted_text.posting_locs = super_posting_locs_text\n",
    "\n",
    "# Add the token - df dictionary to the inverted index\n",
    "inverted_text.df = w2df_dict_text\n",
    "\n",
    "#add document_len\n",
    "inverted_text.document_len = document_len_text\n",
    "\n",
    "# dictionary => {doc_id:title of the documant}\n",
    "inverted_text.doc_id_title = doc_title_pairs_dict\n",
    "\n",
    "inverted_text.doc_id_to_norm = doc_id_to_norm_text\n",
    "\n",
    "inverted_text.write_index('.', 'text_index')\n",
    "# upload to gs\n",
    "index_src = \"text_index.pkl\"\n",
    "index_dst = f'gs://{bucket_name}/postings_gcp/{index_src}'\n",
    "!gsutil cp $index_src $index_dst"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!gsutil ls -lh $index_dst"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}